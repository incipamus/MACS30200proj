---
title: "PS3 Hodgepodge"
author: "HyungJin Cho"
date: "05/15/2017"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

library(tidyverse)
library(modelr)
library(stringr)
library(pander)
library(broom)
library(forcats)
library(car)
library(ISLR)
library(lmtest)
library(Amelia)

theme_set(theme_minimal())
set.seed(1234)
options(digits=4)

```

## 1.Regression diagnostics
### a)Test the model to identify any unusual and/or influential observations. Identify how you would treat these observations moving forward with this research. Note you do not actually have to estimate a new model, just explain what you would do. This could include things like dropping observations, respecifying the model, or collecting additional variables to control for this influential effect.

```{r 1.a}
DF_1 = read_csv("biden.csv")%>%
  rownames_to_column(var="id")
summary(DF_1)

DF_2 = read_csv("biden.csv") %>%
  na.omit() %>%
  rownames_to_column(var="id")
pander(summary(DF_2))

```

Observation Treatment: n/a values are dropped from the dataset to test a model.


### b)Test for non-normally distributed errors. If they are not normally distributed, propose how to correct for them.
```{r 1.b}
MOD_1 = lm(biden ~ age + female + educ, data=DF_2)
tidy(MOD_1)
pander(summary(MOD_1))

car::qqPlot(MOD_1)

augment(MOD_1, DF_2) %>%
  mutate(.student=rstudent(MOD_1)) %>%
  ggplot(aes(.student)) +
  geom_density(adjust=.5) +
  labs(title="Residual Plot",
       x="Studentized Residuals",
       y="Estimated Density")

DF_2 = DF_2 %>%
  mutate(biden_2=biden**1.5)

MOD_2 = lm(biden_2 ~ age + female + educ, data=DF_2)
tidy(MOD_2)
pander(summary(MOD_2))

car::qqPlot(MOD_2)

augment(MOD_2, DF_2) %>%
  mutate(.student = rstudent(MOD_2)) %>%
  ggplot(aes(.student)) +
  geom_density(adjust = .5) +
  labs(title="Residual Plot",
       x="Studentized residuals",
       y="Estimated density")

```

Test for Non-Normally Distributed Errors: The distribution is left-skewed. To correct it, power transformation of 1.5 for the `biden` variable was applied.

### c)Test for heteroscedasticity in the model. If present, explain what impact this could have on inference.
```{r 1.c}
DF_2 = DF_2 %>%
  add_predictions(MOD_1) %>%
  add_residuals(MOD_1)

ggplot(DF_2, aes(pred, resid)) +
  geom_point(alpha=.2) +
  geom_hline(yintercept=0, linetype=2) +
  geom_quantile(method="rqss", lambda=5, quantiles=c(.05,.95)) +
  labs(title="Homoscedastic Variance of Error Terms",
       x="Predicted Values",
       y="Residuals")

bptest(MOD_1)
```

The results of the Breusch-Pagan test show that the null hypothesis of homoskedastic standard errors is rejected. This means that heteroscedasticity exists. This may impact a decrease in reliance on estimated standard error by the regression model.

### d)Test for multicollinearity. If present, propose if/how to solve the problem.
```{r 1.d}
# correlation matrix
cormat_heatmap = function(data){
  cormat = round(cor(data), 2)
  get_upper_tri = function(cormat){
    cormat[lower.tri(cormat)] = NA
    return(cormat)
  }
  upper_tri = get_upper_tri(cormat)
  reorder_cormat = function(cormat){
    dd = as.dist((1-cormat)/2)
    hc = hclust(dd)
    cormat = cormat[hc$order, hc$order]
  }
  cormat = reorder_cormat(cormat)
  upper_tri = get_upper_tri(cormat)
  melted_cormat = reshape2::melt(upper_tri, na.rm = TRUE)
  ggheatmap = ggplot(melted_cormat, aes(Var2, Var1, fill = value)) +
    geom_tile(color = "white") +
    scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                         midpoint = 0, limit = c(-1,1), space = "Lab", 
                         name="Pearson\nCorrelation") +
    theme_minimal() + 
    theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                     size = 12, hjust = 1)) +
    coord_fixed()
  ggheatmap + 
    geom_text(aes(Var2, Var1, label = value), color = "black", size = 4) +
    theme(
      axis.title.x = element_blank(),
      axis.title.y = element_blank(),
      panel.grid.major = element_blank(),
      panel.border = element_blank(),
      panel.background = element_blank(),
      axis.ticks = element_blank(),
      legend.position = "bottom") +
    labs(title="Correlation matrix")
}

cormat_heatmap(select(DF_2, age, female, educ))

library(GGally)
ggpairs(select(DF_2, age, female, educ))

```

The results show that the correlations between variables are low. This indicates there is no multicollinearity problem.

## 2.Interaction terms
### a)Evaluate the marginal effect of age on Joe Biden thermometer rating, conditional on education. Consider the magnitude and direction of the marginal effect, as well as its statistical significance.
```{r 2.a}
MOD_3 = lm(biden ~ age + educ + age*educ, data=DF_2)
tidy(MOD_3)
pander(summary(MOD_3))

#Standard errors for instantaeous effect
instant_effect <- function(model, mod_var){
  int.name = names(model$coefficients)[[which(str_detect(names(model$coefficients), ":"))]]
  marg_var = str_split(int.name, ":")[[1]][[which(str_split(int.name, ":")[[1]] != mod_var)]]
  beta.hat = coef(model)
  cov = vcov(model)
  if(class(model)[[1]] == "lm"){
    z = seq(min(model$model[[mod_var]]), max(model$model[[mod_var]]))
  } else {
    z = seq(min(model$data[[mod_var]]), max(model$data[[mod_var]]))
  }
  dy.dx = beta.hat[[marg_var]] + beta.hat[[int.name]] * z
  se.dy.dx = sqrt(cov[marg_var, marg_var] +
                     z^2 * cov[int.name, int.name] +
                     2 * z * cov[marg_var, int.name])
  data_frame(z = z,
             dy.dx = dy.dx,
             se = se.dy.dx)
}

#Range plot
instant_effect(MOD_3, "educ") %>%
  ggplot(aes(z, dy.dx,
             ymin = dy.dx - 1.96 * se,
             ymax = dy.dx + 1.96 * se)) +
  geom_pointrange() +
  geom_hline(yintercept = 0, linetype = 2) +
  labs(title = "Marginal Effect of Age",
       subtitle = "By Education",
       x = "Education",
       y = "Estimated Marginal Effect")

#Line plot
instant_effect(MOD_3, "educ") %>%
  ggplot(aes(z, dy.dx)) +
  geom_line() +
  geom_line(aes(y = dy.dx - 1.96 * se), linetype = 2) +
  geom_line(aes(y = dy.dx + 1.96 * se), linetype = 2) +
  geom_hline(yintercept = 0) +
  labs(title = "Marginal Effect of Age",
       subtitle = "By Education",
       x = "Education",
       y = "Estimated Marginal Effect")

linearHypothesis(MOD_3, "age + age:educ")

```

The results show that marginal effect of age goes down when education goes up. The hypothesis test indicates the marginal effect is not significant at the education of 13 to 16.

### b)Evaluate the marginal effect of education on Joe Biden thermometer rating, conditional on age. Consider the magnitude and direction of the marginal effect, as well as its statistical significance.
```{r 2.b}
#Range plot
instant_effect(MOD_3, "age") %>%
  ggplot(aes(z, dy.dx,
             ymin = dy.dx - 1.96 * se,
             ymax = dy.dx + 1.96 * se)) +
  geom_pointrange() +
  geom_hline(yintercept = 0, linetype = 2) +
  labs(title = "Marginal Effect of Education",
       subtitle = "By Age",
       x = "Age",
       y = "Estimated Marginal Effect")

#Line plot
instant_effect(MOD_3, "age") %>%
  ggplot(aes(z, dy.dx)) +
  geom_line() +
  geom_line(aes(y = dy.dx - 1.96 * se), linetype = 2) +
  geom_line(aes(y = dy.dx + 1.96 * se), linetype = 2) +
  geom_hline(yintercept = 0) +
  labs(title = "Marginal Effect of Education",
       subtitle = "By Age",
       x = "Age",
       y = "Estimated Marginal Effect")

linearHypothesis(MOD_3, "educ + age:educ")

```

The results show that marginal effect of education goes down when education goes up. The hypothesis test indicates the marginal effect is not significant at the age under 45.

## 3.Missing data
```{r 3.}
MISSING = amelia(as.data.frame(DF_1), m=5, idvars=c("id"))

missmap(MISSING)

GGally::ggpairs(select_if(DF_1, is.numeric))

models_imp = function(data){
  data_frame(data) %>%
  mutate(model=map(data, ~ lm(biden ~ age + female + educ,
                              data = .x)),
         coef = map(model, tidy)) %>%
  unnest(coef, .id = "id")
}
models_imp_1 = models_imp(MISSING$imputations)

MISSING_2 = amelia(as.data.frame(DF_1), m=5, idvars=c("id"),
                   logs=c("age"),
                   sqrts=c("biden"),
                   noms=c("dem", "rep", "female"))

models_imp_2 = models_imp(MISSING_2$imputations)

mi.meld.plus = function(df_tidy){
  coef.out = df_tidy %>%
    select(id:estimate) %>%
    spread(term, estimate) %>%
    select(-id)
  se.out = df_tidy %>%
    select(id, term, std.error) %>%
    spread(term, std.error) %>%
    select(-id)
  combined.results = mi.meld(q = coef.out, se = se.out)
  data_frame(term = colnames(combined.results$q.mi),
             estimate.mi = combined.results$q.mi[1, ],
             std.error.mi = combined.results$se.mi[1, ])
}

tidy(MOD_1) %>%
  left_join(mi.meld.plus(models_imp_2)) %>%
  select(-statistic, -p.value)

bind_rows(orig = tidy(MOD_1),
          full_imp = mi.meld.plus(models_imp_1) %>%
            rename(estimate = estimate.mi,
                   std.error = std.error.mi),
          trans_imp = mi.meld.plus(models_imp_2) %>%
            rename(estimate = estimate.mi,
                   std.error = std.error.mi),
          .id = "method") %>%
  mutate(method = factor(method, levels = c("orig", "full_imp", "trans_imp"),
                         labels = c("Listwise deletion", "Full imputation",
                                    "Transformed imputation")),
         term = factor(term, levels = c("(Intercept)", "age", "female", "educ"),
                       labels = c("(Intercept)", "Age", "Female", "Education"))) %>%
  filter(term != "(Intercept)") %>%
  ggplot(aes(fct_rev(term), estimate, color = fct_rev(method),
             ymin = estimate - 1.96 * std.error,
             ymax = estimate + 1.96 * std.error)) +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_pointrange(position = position_dodge(.75)) +
  coord_flip() +
  scale_color_discrete(guide = guide_legend(reverse = TRUE)) +
  labs(title = "Comparing regression results",
       x = NULL,
       y = "Estimated parameter",
       color = NULL) +
  theme(legend.position = "bottom")

```

Missing values occur highly in the variable `baiden`. Log transformation to `age` and squared-root transformation to `biden` were applied for Amelia's Multiple Imputation. The results show that absolute value of the intercept and parameter of `female`, `educ` decreased with a higher standard error and parameter of `age` increased with a lower standard error in an imputed model compared to the original, non-imputed model.
